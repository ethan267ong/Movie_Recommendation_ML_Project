{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rWprYhb1lUUA"
      },
      "id": "rWprYhb1lUUA"
    },
    {
      "cell_type": "markdown",
      "id": "e5495f6f",
      "metadata": {
        "id": "e5495f6f"
      },
      "source": [
        "# BT4222: Neural Matrix Factorization (NeuMF) for Movie Recommendation\n",
        "## Introduction\n",
        "This notebook presents the development and evaluation of a Neural Matrix Factorization (NeuMF) model for personalized movie recommendation. NeuMF is a hybrid deep learning architecture that combines the strengths of generalized matrix factorization (GMF) and multi-layer perceptrons (MLP) to effectively model both linear and nonlinear user-item interaction patterns.\n",
        "\n",
        "In this architecture, user and item embeddings are passed through two parallel paths:\n",
        "\n",
        "- The GMF path performs element-wise interactions to capture linear relationships\n",
        "\n",
        "- The MLP path concatenates embeddings and processes them through stacked dense layers to learn complex, high-order patterns.\n",
        "\n",
        "Alongside these two paths, metadata embeddings are also incorporated into the MLP path, allowing the model to learn from additional item-specific features. These embeddings are learned from metadata attributes like director, main cast, genre, and language, enriching the model’s understanding of item relationships beyond user-item interactions.\n",
        "\n",
        "The outputs from both the GMF and MLP paths, along with the metadata embeddings, are then fused and passed through a final prediction layer. The entire model is trained end-to-end using PyTorch on explicit rating data.\n",
        "\n",
        "To assess model performance, we evaluate:\n",
        "\n",
        "- RMSE for measuring the accuracy of predicted ratings, and\n",
        "\n",
        "- Top-K ranking metrics such as Precision@K, Recall@K, NDCG@K, MAP@K, Hit Rate, and MRR, which reflect the quality of the ranked recommendations across users.\n",
        "\n",
        "- AUC which evaluates the model’s ability to distinguish between relevant (positive) and irrelevant (negative) items across users"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97f56e0",
      "metadata": {
        "id": "b97f56e0"
      },
      "source": [
        "# Mount Google Drive\n",
        "This code cell mounts your Google Drive in the Colab environment, enabling access to datasets stored in your drive for subsequent processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23dfabe9",
      "metadata": {
        "id": "23dfabe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b44f30-2ee4-4d85-df8b-755ad388b338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xh2B1p0x_AQw",
      "metadata": {
        "id": "xh2B1p0x_AQw"
      },
      "source": [
        "## Import Libraries and Load Raw Datasets\n",
        "Importing necessary libraries.\n",
        "Loading all primary datasets used in the project:\n",
        "\n",
        "- `df_tmdb_final.csv`: Final movie metadata with embeddings\n",
        "\n",
        "- `df_links_with_ratings.csv`: User ratings merged with TMDB movie IDs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d042c1d",
      "metadata": {
        "id": "3d042c1d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "import tqdm as tqdm\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "import random\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "df_tmdb_final = pd.read_csv('/content/drive/MyDrive/BT4222_Project/df_tmdb_final.csv')\n",
        "df_links_with_ratings = pd.read_csv('/content/drive/MyDrive/BT4222_Project/df_links_with_ratings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Prepare Content Feature Dataset\n",
        "We create a new dataframe df_content by selecting only the embeddings column needed for this model:\n",
        "\n",
        "- `movie_id`: Unique identifier for each movie  \n",
        "- `original_title`: Movie title  \n",
        "- `weighted_vote_score`: Popularity/review score used for ranking  \n",
        "- `genre`: Genre tags (typically as lists or encoded vectors)  \n",
        "- `director_embedding`: Numeric embedding representing the movie’s director  \n",
        "- `main_cast_embeddings`: Averaged or stacked embeddings of main cast members  \n",
        "- `production_company_embedding`: Embedding of the associated production company  \n",
        "- `original_language_embedding`: Embedding for the movie's original language  \n",
        "\n",
        "Duplicate entries are removed based on `movie_id` to ensure each movie appears only once.\n",
        "\n",
        "### 1.1 String Cleaning through parse_embedding_from_array_wrapper\n",
        "When the embeddings were extracted and saved during data cleaning, they were stored as strings in the CSV. These strings sometimes include extra wrappers, such as array( ... ), and have numbers separated by spaces instead of commas—this is not a valid Python literal.\n",
        "\n",
        "- **Removing Array Wrappers**:\n",
        "It uses a regular expression to detect and remove the \"array(...)\" wrapper so that only the inner content remains.\n",
        "\n",
        "- **Ensuring Proper Bracketing**:\n",
        "It makes sure that the cleaned string starts with [ and ends with ] so that it represents a valid Python list literal.\n",
        "\n",
        "- **Inserting Commas**:\n",
        "It then uses another regular expression to insert commas between numbers that are only separated by whitespace\n",
        "\n",
        "### 1.2 Defining parse_embedding\n",
        "When the embeddings were extracted and saved during the data cleaning process, they were converted to strings in the CSV file. This makes it necessary to parse these string representations back into numerical arrays before further processing. The parse_embedding function handles this conversion by:\n",
        "\n",
        "- **Checking if the input is a string**:\n",
        "It may be a string representation of a list (e.g., \"[0.1, 0.2, 0.3]\"), so we need to convert it back to a Python list and then into a NumPy array.\n",
        "\n",
        "- **Handling errors gracefully**:\n",
        "If the string cannot be converted (due to formatting issues), the function returns a zero vector with the specified dimension as a fallback.\n",
        "\n",
        "- **Working with lists or arrays**:\n",
        "If the embedding is already in list or array format, it converts (or reaffirms) it as a NumPy array."
      ],
      "metadata": {
        "id": "AMip-R39XrMh"
      },
      "id": "AMip-R39XrMh"
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1: Preparing the Dataset\n",
        "\n",
        "# Select relevant content-based features\n",
        "df_content = df_tmdb_final[[\n",
        "    'movie_id',\n",
        "    'original_title',\n",
        "    'weighted_vote_score',\n",
        "    'genre',\n",
        "    'director_embedding',\n",
        "    'main_cast_embeddings',\n",
        "    'production_company_embedding',\n",
        "    'original_language_embedding'\n",
        "]].drop_duplicates(subset='movie_id').reset_index(drop=True)\n",
        "\n",
        "\n",
        "def parse_embedding_from_array_wrapper(s, dim=300):\n",
        "    \"\"\"\n",
        "    Convert messy string embeddings (with array(...) wrappers or irregular formatting)\n",
        "    into fixed-length NumPy vectors.\n",
        "    \"\"\"\n",
        "    if pd.isna(s):\n",
        "        return np.zeros(dim)\n",
        "\n",
        "    # Clean wrappers and whitespace\n",
        "    s = re.sub(r'array\\((.*?)\\)', r'\\1', s)      # Remove 'array(...)'\n",
        "    s = s.replace('\\n', ' ')                     # Remove newlines\n",
        "    s = s.replace('[', '').replace(']', '')      # Strip brackets\n",
        "\n",
        "    # Extract floats using regex (handles scientific notation too)\n",
        "    numbers = re.findall(r'-?\\d+\\.\\d+(?:e[+-]?\\d+)?', s)\n",
        "\n",
        "    try:\n",
        "        vec = np.array([float(n) for n in numbers], dtype=np.float32)\n",
        "\n",
        "        # Pad or truncate to target dimension\n",
        "        if len(vec) < dim:\n",
        "            vec = np.pad(vec, (0, dim - len(vec)), mode='constant')\n",
        "        elif len(vec) > dim:\n",
        "            vec = vec[:dim]\n",
        "\n",
        "        return vec\n",
        "\n",
        "    except:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "\n",
        "def parse_embedding(x, dim=300):\n",
        "    \"\"\"\n",
        "    Parses an embedding from either a valid string or an actual list/array.\n",
        "    \"\"\"\n",
        "    if isinstance(x, str):\n",
        "        x = x.strip()\n",
        "        if x.startswith('[') and x.endswith(']') and 'array' not in x:\n",
        "            try:\n",
        "                return np.array(ast.literal_eval(x))\n",
        "            except:\n",
        "                return np.zeros(dim)\n",
        "    elif isinstance(x, (list, np.ndarray)):\n",
        "        return np.array(x)\n",
        "\n",
        "    return np.zeros(dim)\n",
        "\n",
        "\n",
        "\n",
        "embedding_cols = [\n",
        "    'main_cast_embeddings',\n",
        "    'director_embedding',\n",
        "    'production_company_embedding',\n",
        "    'original_language_embedding',\n",
        "]\n",
        "\n",
        "# Apply wrapper-based parsing to messy embedding strings\n",
        "for col in embedding_cols:\n",
        "    df_content[col] = df_content[col].apply(lambda v: parse_embedding_from_array_wrapper(v, dim=300))\n",
        "\n",
        "# Apply standard parsing to clean genre embeddings\n",
        "df_content['genre'] = df_content['genre'].apply(lambda v: parse_embedding(v, dim=300))\n",
        "\n",
        "df_merged = pd.merge(df_links_with_ratings, df_content, left_on='tmdbId', right_on='movie_id', how='inner')\n"
      ],
      "metadata": {
        "id": "R6nlNFfyU3k1"
      },
      "id": "R6nlNFfyU3k1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_cols = [\n",
        "    'main_cast_embeddings',\n",
        "    'director_embedding',\n",
        "    'production_company_embedding',\n",
        "    'original_language_embedding',\n",
        "    'genre'\n",
        "]\n",
        "\n",
        "for col in embedding_cols:\n",
        "    print(f\"{col} — valid (non-zero) vectors:\", df_content[col].apply(lambda x: np.sum(x) > 0).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbuWM5Y8U6mP",
        "outputId": "18d8d092-78e4-43a8-a8b2-ef15b6923fba"
      },
      "id": "HbuWM5Y8U6mP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main_cast_embeddings — valid (non-zero) vectors: 268\n",
            "director_embedding — valid (non-zero) vectors: 1216\n",
            "production_company_embedding — valid (non-zero) vectors: 1341\n",
            "original_language_embedding — valid (non-zero) vectors: 1418\n",
            "genre — valid (non-zero) vectors: 1446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Combining embeddings and Extracting the Relevant Columns\n",
        "This code combines multiple feature embeddings (such as main cast, director, production company, genre, and original language) into a single metadata embedding vector for each movie. It concatenates the embeddings for each movie and stores them in the `combined_metadata` column. The final dataset, `df_ratings`, includes userId, movieId, rating, and combined_metadata. The code also checks for consistent embedding lengths and prints the dimension of the combined metadata vector, ensuring that all vectors have the same length for model input."
      ],
      "metadata": {
        "id": "PwZk590Tvup9"
      },
      "id": "PwZk590Tvup9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all into a single metadata embedding vector\n",
        "df_merged['combined_metadata'] = df_merged.apply(\n",
        "    lambda row: np.concatenate([row['main_cast_embeddings'].astype(np.float32),\n",
        "                               row['director_embedding'].astype(np.float32),\n",
        "                               row['production_company_embedding'].astype(np.float32),\n",
        "                               row['genre'].astype(np.float32),\n",
        "                               row['original_language_embedding'].astype(np.float32)]),\n",
        "    axis=1\n",
        ")\n",
        "# Final dataset to be used, columns extracted\n",
        "df_ratings = df_merged[['userId', 'movieId', 'rating', 'combined_metadata']]\n",
        "\n",
        "# Checks for embedding lengths and print dimension of metadata vector\n",
        "embedding_lengths = df_ratings['combined_metadata'].apply(lambda x: len(x))\n",
        "assert embedding_lengths.nunique() == 1, \"Inconsistent embedding lengths detected!\"\n",
        "metadata_dim = embedding_lengths.iloc[0]\n",
        "print(f\"Combined metadata dimension: {metadata_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBj0D1BQIzUM",
        "outputId": "46a6e4ea-cda5-481d-8a38-d990f9647658"
      },
      "id": "RBj0D1BQIzUM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined metadata dimension: 1246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UcjNXL57BZG5",
      "metadata": {
        "id": "UcjNXL57BZG5"
      },
      "source": [
        "## Step 2: Splitting User Interaction Data into Train and Test Sets by Ratio\n",
        "To ensure a fair and user-centric evaluation, we split the ratings data per user, such that 80% of each user’s ratings are allocated for training and the remaining 20% for testing. This strategy ensures that every user is represented in both sets, and that the model is evaluated on items it has not seen for that specific user.\n",
        "\n",
        "The splitting is randomized but reproducible via a fixed random seed, and guarantees at least one test item per user, avoiding cold-start issues during evaluation.\n",
        "\n",
        "Outputs:\n",
        "\n",
        "- `train_df`: User-item interaction data used for training the model\n",
        "- `test_df`: Held-out interactions used for Top-K recommendation evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6MTmUNQKC4uc",
      "metadata": {
        "id": "6MTmUNQKC4uc"
      },
      "outputs": [],
      "source": [
        "def user_train_test_split_by_ratio(df, test_ratio=0.2, user_col='userId', item_col='movieId', random_state=42):\n",
        "    \"\"\"\n",
        "    Splits each user's interactions into train and test sets by percentage.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input user-item interaction DataFrame.\n",
        "        test_ratio (float): Proportion of each user’s interactions to hold out for test.\n",
        "        user_col (str): User ID column name.\n",
        "        item_col (str): Item ID column name.\n",
        "        random_state (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        train_df, test_df: DataFrames split per user.\n",
        "    \"\"\"\n",
        "    train_list = []\n",
        "    test_list = []\n",
        "\n",
        "    for user_id, user_data in df.groupby(user_col):\n",
        "        n_test = max(1, int(len(user_data) * test_ratio))\n",
        "        if len(user_data) <= n_test:\n",
        "            continue\n",
        "        test = user_data.iloc[:n_test]\n",
        "        train = user_data.iloc[n_test:]\n",
        "        test_list.append(test)\n",
        "        train_list.append(train)\n",
        "\n",
        "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
        "    test_df = pd.concat(test_list).reset_index(drop=True)\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = user_train_test_split_by_ratio(df_ratings, test_ratio=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9fa711c",
      "metadata": {
        "id": "d9fa711c"
      },
      "source": [
        "## Step 3: Neural Matrix Factorization (NeuMF) with PyTorch\n",
        "In this step, we implement the Neural Matrix Factorization (NeuMF) model using PyTorch. NeuMF is a hybrid deep learning architecture that combines Generalized Matrix Factorization (GMF) and Multi-Layer Perceptrons (MLP) to model both linear and non-linear user-item interaction patterns.\n",
        "\n",
        "The model is designed to learn from both explicit user-item interactions (ratings) and additional metadata embeddings. Specifically, it integrates two main components:\n",
        "\n",
        "- GMF (Generalized Matrix Factorization): This path captures linear relationships between users and items by learning low-dimensional embeddings for users and items, and then performing element-wise interactions between them.\n",
        "\n",
        "- MLP (Multi-Layer Perceptron): This path learns non-linear interaction patterns by concatenating the embeddings of users, items, and metadata, followed by a series of dense layers to learn complex relationships. This helps the model capture high-order interactions.\n",
        "\n",
        "These two paths are combined, and the output is passed through a final layer that predicts the rating for each user-item pair. Additionally, `metadata` embeddings (such as director, cast, genre, language, etc.) are incorporated into the MLP path to enrich the representation of items, making the model more robust to data sparsity and improving the recommendation quality.\n",
        "\n",
        "Components:\n",
        "- `RatingDataset`: A custom PyTorch dataset that handles batching of user-item-rating-triplets, ensuring efficient data loading during training.\n",
        "\n",
        "- `NeuMFWithMetadata class`: The main model class, integrating GMF and MLP components, along with metadata processing. This model is capable of learning from both user-item interactions and metadata embeddings.\n",
        "\n",
        "Outputs:\n",
        "A fully compiled NeuMF model that can be trained on batches of user-item interactions and metadata. The model predicts ratings by learning from both shallow linear interactions (GMF) and deep non-linear patterns (MLP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef242201",
      "metadata": {
        "id": "ef242201"
      },
      "outputs": [],
      "source": [
        "class RatingDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = df['user'].values\n",
        "        self.items = df['item'].values\n",
        "        self.ratings = df['rating'].values\n",
        "        self.metadata = np.stack(df['combined_metadata'].values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.metadata[idx], self.ratings[idx]\n",
        "\n",
        "\n",
        "class NeuMFWithMetadata(nn.Module):\n",
        "    def __init__(self, n_users, n_items, mf_dim=32, mlp_dims=[128, 64, 32], metadata_dim=800):\n",
        "        super(NeuMFWithMetadata, self).__init__()\n",
        "\n",
        "        # GMF embeddings (no metadata)\n",
        "        self.mf_user_embed = nn.Embedding(n_users, mf_dim)\n",
        "        self.mf_item_embed = nn.Embedding(n_items, mf_dim)\n",
        "\n",
        "        # MLP embeddings\n",
        "        self.mlp_user_embed = nn.Embedding(n_users, mlp_dims[0] // 4)\n",
        "        self.mlp_item_embed = nn.Embedding(n_items, mlp_dims[0] // 4)\n",
        "\n",
        "        # Input size to MLP = user_embed + item_embed + metadata_emb\n",
        "        mlp_input_size = (mlp_dims[0] // 2) + metadata_dim\n",
        "\n",
        "        # MLP layers\n",
        "        mlp_layers = []\n",
        "        input_size = mlp_input_size\n",
        "        for dim in mlp_dims:\n",
        "            mlp_layers.append(nn.Linear(input_size, dim))\n",
        "            mlp_layers.append(nn.ReLU())\n",
        "            input_size = dim\n",
        "        self.mlp = nn.Sequential(*mlp_layers)\n",
        "\n",
        "        # Final output layer (GMF + MLP output)\n",
        "        self.output_layer = nn.Linear(mf_dim + mlp_dims[-1], 1)\n",
        "\n",
        "    def forward(self, user, item, metadata_embedding):\n",
        "        # GMF path (no metadata)\n",
        "        mf_user = self.mf_user_embed(user)\n",
        "        mf_item = self.mf_item_embed(item)\n",
        "        mf_vector = mf_user * mf_item\n",
        "\n",
        "        # MLP path (user + item + metadata)\n",
        "        mlp_user = self.mlp_user_embed(user)\n",
        "        mlp_item = self.mlp_item_embed(item)\n",
        "        mlp_input = torch.cat([mlp_user, mlp_item, metadata_embedding], dim=-1)  # Concatenate metadata\n",
        "        mlp_vector = self.mlp(mlp_input)\n",
        "\n",
        "        # Combine both paths\n",
        "        combined = torch.cat([mf_vector, mlp_vector], dim=-1)\n",
        "        prediction = self.output_layer(combined).squeeze()\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abkqCWPmo75a",
      "metadata": {
        "id": "abkqCWPmo75a"
      },
      "source": [
        "## Step 4: Training the Neural Matrix Factorization (NeuMF) Model\n",
        "In this step, we train the Neural Matrix Factorization (NeuMF) model, which combines the strengths of Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP), while incorporating metadata embeddings to enhance prediction accuracy.\n",
        "\n",
        "User and item IDs are first label-encoded into integer indices, which are then used to create a custom `RatingDataset`. The dataset is loaded into PyTorch `DataLoader`s for efficient mini-batch training. The model is trained with Mean Squared Error (MSE) loss, which measures the discrepancy between predicted and actual ratings, and optimized using the Adam optimizer.\n",
        "\n",
        "During training:\n",
        "\n",
        "- The model processes batches of user-item-rating triplets.\n",
        "\n",
        "- It computes predictions through the GMF and MLP paths, which are later combined.\n",
        "\n",
        "- Metadata embeddings (e.g., movie genre, director, cast) are also used in the MLP path to provide additional context, helping the model learn richer item representations.\n",
        "\n",
        "- The model performs backpropagation and updates the parameters after each batch.\n",
        "\n",
        "This process is repeated over multiple epochs, allowing the model to converge and better fit the training data.\n",
        "\n",
        "Components:\n",
        "- `NeuMF Model`: Combines GMF and MLP components, with metadata embeddings incorporated into the MLP path.\n",
        "\n",
        "- `DataLoader`: Efficiently streams batches of user-item-rating data for training.\n",
        "\n",
        "- `MSE Loss`: Guides the model to minimize prediction error on ratings.\n",
        "\n",
        "- `Adam Optimizer`: An adaptive optimization algorithm used to fine-tune the model’s parameters.\n",
        "\n",
        "Output:\n",
        "A fully trained NeuMF model that learns to predict ratings by modeling both shallow linear interactions (via GMF) and deep non-linear patterns (via MLP), with enriched metadata features. The average loss printed after each epoch shows how well the model is fitting the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d54ddd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d54ddd",
        "outputId": "ac8c7a3f-fed7-4997-aafe-9fb89bd57aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-188bb2b8172b>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_ratings['user'] = le_user.fit_transform(df_ratings['userId'])\n",
            "<ipython-input-8-188bb2b8172b>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_ratings['item'] = le_item.fit_transform(df_ratings['movieId'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Avg Loss: 0.8428\n",
            "Epoch 2, Avg Loss: 0.7275\n",
            "Epoch 3, Avg Loss: 0.7046\n",
            "Epoch 4, Avg Loss: 0.6928\n",
            "Epoch 5, Avg Loss: 0.6796\n"
          ]
        }
      ],
      "source": [
        "le_user = LabelEncoder()\n",
        "le_item = LabelEncoder()\n",
        "\n",
        "df_ratings.loc[:, 'user'] = le_user.fit_transform(df_ratings['userId'])\n",
        "df_ratings.loc[:, 'item'] = le_item.fit_transform(df_ratings['movieId'])\n",
        "\n",
        "n_users = df_ratings['user'].nunique()\n",
        "n_items = df_ratings['item'].nunique()\n",
        "\n",
        "# Apply label encoding to split sets\n",
        "train_df['user'] = le_user.transform(train_df['userId'])\n",
        "train_df['item'] = le_item.transform(train_df['movieId'])\n",
        "\n",
        "test_df['user'] = le_user.transform(test_df['userId'])\n",
        "test_df['item'] = le_item.transform(test_df['movieId'])\n",
        "\n",
        "train_dataset = RatingDataset(train_df)\n",
        "test_dataset = RatingDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuMFWithMetadata(n_users, n_items, metadata_dim=metadata_dim).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Train loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for users, items, metadata, ratings in train_loader:\n",
        "        users = users.to(device)\n",
        "        items = items.to(device)\n",
        "        metadata = metadata.float().to(device)\n",
        "        ratings = ratings.float().to(device)\n",
        "\n",
        "        preds = model(users, items, metadata)\n",
        "        loss = criterion(preds, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38e33755",
      "metadata": {
        "id": "38e33755"
      },
      "source": [
        "## Step 5: NeuMF Model Prediction and RMSE Evaluation\n",
        "In this step, we evaluate the performance of the trained NeuMF model by generating rating predictions for the test set and calculating the Root Mean Squared Error (RMSE). RMSE measures the average deviation between predicted and actual user ratings, offering a quantitative assessment of the model’s prediction accuracy.\n",
        "\n",
        "We begin by switching the model to evaluation mode (`model.eval()`) and disabling gradient computation (`torch.no_grad()`) for efficiency. The model then generates predictions for all user-item pairs in the test set. These predicted ratings are compared with the ground-truth ratings to assess how well the model has learned user preferences.\n",
        "\n",
        "The predicted ratings, along with their corresponding user and item IDs, are stored in a DataFrame. The label encoders are used to reverse the encoding and convert the indices back to the original userId and movieId for readability.\n",
        "\n",
        "Steps Involved:\n",
        "- Switch the model to evaluation mode (`model.eval()`) and disable gradients (`torch.no_grad()`).\n",
        "\n",
        "- Generate predictions for each user-item pair in the test set.\n",
        "\n",
        "- Collect predictions and actual ratings into a DataFrame.\n",
        "\n",
        "- Reverse the label encoding for userId and movieId to return the original values.\n",
        "\n",
        "- Calculate RMSE to measure the prediction error.\n",
        "\n",
        "Outputs:\n",
        "- `neumf_df`: A DataFrame containing userId, movieId, true rating, and predicted rating.\n",
        "\n",
        "- `rmse_neumf`: A scalar value representing the average RMSE on the test set.\n",
        "\n",
        "This RMSE score serves as an interpretable metric for evaluating how well the NeuMF model predicts user ratings and how closely it approximates actual user preferences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f685aa4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f685aa4e",
        "outputId": "da617c12-ae9a-4157-ae5c-2239afcea100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuMF with Metadata RMSE: 0.8813\n"
          ]
        }
      ],
      "source": [
        "# Prepare NeuMF predictions\n",
        "model.eval()\n",
        "preds = []\n",
        "true_ratings = []\n",
        "user_ids = []\n",
        "item_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for users, items, metadata, ratings in test_loader:\n",
        "        users = users.to(device)\n",
        "        items = items.to(device)\n",
        "        metadata = metadata.float().to(device)\n",
        "\n",
        "        outputs = model(users, items, metadata).cpu().numpy()\n",
        "        preds.extend(outputs)\n",
        "        true_ratings.extend(ratings.numpy())\n",
        "        user_ids.extend(users.cpu().numpy())\n",
        "        item_ids.extend(items.cpu().numpy())\n",
        "\n",
        "# Build results DataFrame\n",
        "rmse_df = pd.DataFrame({\n",
        "    'user': user_ids,\n",
        "    'item': item_ids,\n",
        "    'true_rating': true_ratings,\n",
        "    'predicted_rating': preds\n",
        "})\n",
        "\n",
        "# Inverse label encoding for readability\n",
        "rmse_df['userId'] = le_user.inverse_transform(rmse_df['user'])\n",
        "rmse_df['movieId'] = le_item.inverse_transform(rmse_df['item'])\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(mean_squared_error(rmse_df['true_rating'], rmse_df['predicted_rating']))\n",
        "print(f\"NeuMF with Metadata RMSE: {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0tzb5p_NqWd4",
      "metadata": {
        "id": "0tzb5p_NqWd4"
      },
      "source": [
        "## Step 6: Batched Rank-All Evaluation of the Model\n",
        "In this step, we implement a batched Rank-All evaluation procedure to assess the performance of the trained Neural Collaborative Filtering (NCF) model, which incorporates metadata embeddings in the prediction process. This evaluation simulates a realistic recommendation scenario where the model ranks all unseen items for each user and recommends the Top-K most relevant items.\n",
        "\n",
        "To improve computational efficiency, scores for all candidate items are computed in batches using PyTorch tensor operations. For each user, the model computes scores for all candidate items (which are the items not seen during training) and also includes the test items in the candidate pool to ensure the model has the opportunity to rank them correctly.\n",
        "\n",
        "This evaluation is done using the following Top-K ranking metrics:\n",
        "\n",
        "- Precision@K: The proportion of items in the top-K recommendations that are relevant (i.e., items in the test set for the user).\n",
        "\n",
        "- Recall@K: The proportion of the user’s relevant items that appear in the top-K recommendations.\n",
        "\n",
        "- Hit Rate@K: The proportion of users who received at least one relevant recommendation in their top-K list.\n",
        "\n",
        "- MRR@K (Mean Reciprocal Rank): Measures the ranking of the first relevant item in the top-K list.\n",
        "\n",
        "- MAP@K (Mean Average Precision): Averages the precision values at the ranks where relevant items appear in the top-K list.\n",
        "\n",
        "- NDCG@K (Normalized Discounted Cumulative Gain): Penalizes relevant items ranked lower and rewards those ranked higher in the list.\n",
        "\n",
        "The evaluation leverages metadata embeddings (e.g., director, cast, genre) as part of the model’s predictions, which help improve recommendation accuracy in sparse scenarios. The model is evaluated on both the train and test sets to ensure generalization and robust performance.\n",
        "\n",
        "Output:\n",
        "A printed summary of the mean Top-K metrics across all users, reflecting the model’s ability to rank and recommend relevant items when faced with a large number of candidate items. These metrics help evaluate how well the Neural Collaborative Filtering model, with metadata, performs in a full ranking scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dwUCymZQgTFK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwUCymZQgTFK",
        "outputId": "505521af-69bb-43a3-9770-37ef1108121b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's performance on Test Set\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rank-All Evaluation (With Metadata): 100%|██████████| 215586/215586 [1:07:15<00:00, 53.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision@10: 0.1048\n",
            "Recall@10: 0.1943\n",
            "Hit Rate@10: 0.4647\n",
            "MRR@10: 0.2782\n",
            "MAP@10: 0.1019\n",
            "NDCG@10: 0.1885\n",
            "Model's performance on Train Set\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rank-All Evaluation (With Metadata): 100%|██████████| 215586/215586 [1:10:18<00:00, 51.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision@10: 0.0983\n",
            "Recall@10: 0.0760\n",
            "Hit Rate@10: 0.5069\n",
            "MRR@10: 0.1993\n",
            "MAP@10: 0.0335\n",
            "NDCG@10: 0.1125\n"
          ]
        }
      ],
      "source": [
        "# === Build user-item interaction dictionaries from DataFrame ===\n",
        "def get_user_item_dict(df, user_col='user', item_col='item'):\n",
        "    user_item_dict = defaultdict(set)\n",
        "    for row in df.itertuples():\n",
        "        user_item_dict[getattr(row, user_col)].add(getattr(row, item_col))\n",
        "    return user_item_dict\n",
        "\n",
        "# Build a mapping from item index to metadata vector\n",
        "item_metadata_dict = dict(zip(\n",
        "    df_ratings['item'],  # this is after label encoding\n",
        "    df_ratings['combined_metadata']\n",
        "))\n",
        "\n",
        "# === Batched Top-K Evaluation ===\n",
        "def evaluate_rank_all_batched_with_metadata(model, train_dict, test_dict, n_items, item_metadata_dict, K=10, device='cpu'):\n",
        "    precision_list, recall_list, hit_list, ndcg_list, map_list, mrr_list = [], [], [], [], [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for user in tqdm(test_dict.keys(), desc=\"Rank-All Evaluation (With Metadata)\"):\n",
        "            test_items = test_dict[user]\n",
        "            if len(test_items) == 0:\n",
        "                continue\n",
        "\n",
        "            candidate_items = list(set(range(n_items)) - train_dict[user])\n",
        "            for item in test_items:\n",
        "                if item not in candidate_items:\n",
        "                    candidate_items.append(item)\n",
        "\n",
        "            # Prepare input tensors\n",
        "            user_tensor = torch.tensor([user] * len(candidate_items), dtype=torch.long).to(device)\n",
        "            item_tensor = torch.tensor(candidate_items, dtype=torch.long).to(device)\n",
        "\n",
        "            metadata_tensor = torch.stack([torch.tensor(item_metadata_dict[i]) for i in candidate_items]).float().to(device)\n",
        "\n",
        "            scores = model(user_tensor, item_tensor, metadata_tensor).cpu().numpy()\n",
        "            ranked_items = list(zip(candidate_items, scores))\n",
        "\n",
        "            # Top-K selection\n",
        "            top_k = heapq.nlargest(K, ranked_items, key=lambda x: x[1])\n",
        "            top_k_items = set([i[0] for i in top_k])\n",
        "            hits = [item for item in test_items if item in top_k_items]\n",
        "\n",
        "            # Metrics\n",
        "            precision_list.append(len(hits) / K)\n",
        "            recall_list.append(len(hits) / len(test_items))\n",
        "            hit_list.append(1.0 if hits else 0.0)\n",
        "\n",
        "            ap = 0.0\n",
        "            for idx, (item_id, _) in enumerate(top_k):\n",
        "                if item_id in test_items:\n",
        "                    ap += len([i for i, (itm, _) in enumerate(top_k[:idx+1]) if itm in test_items]) / (idx+1)\n",
        "            map_list.append(ap / len(test_items))\n",
        "\n",
        "            rr = 0.0\n",
        "            for idx, (item_id, _) in enumerate(top_k):\n",
        "                if item_id in test_items:\n",
        "                    rr = 1.0 / (idx + 1)\n",
        "                    break\n",
        "            mrr_list.append(rr)\n",
        "\n",
        "            dcg = sum([1.0 / np.log2(idx + 2) for idx, (itm, _) in enumerate(top_k) if itm in test_items])\n",
        "            idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(test_items), K))])\n",
        "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "            ndcg_list.append(ndcg)\n",
        "\n",
        "    print(f\"\\nPrecision@{K}: {np.mean(precision_list):.4f}\")\n",
        "    print(f\"Recall@{K}: {np.mean(recall_list):.4f}\")\n",
        "    print(f\"Hit Rate@{K}: {np.mean(hit_list):.4f}\")\n",
        "    print(f\"MRR@{K}: {np.mean(mrr_list):.4f}\")\n",
        "    print(f\"MAP@{K}: {np.mean(map_list):.4f}\")\n",
        "    print(f\"NDCG@{K}: {np.mean(ndcg_list):.4f}\")\n",
        "\n",
        "\n",
        "train_user_items = get_user_item_dict(train_df)\n",
        "test_user_items = get_user_item_dict(test_df)\n",
        "\n",
        "print(\"Model's performance on Test Set\\n\")\n",
        "evaluate_rank_all_batched_with_metadata(\n",
        "    model=model,\n",
        "    train_dict=train_user_items,\n",
        "    test_dict=test_user_items,\n",
        "    item_metadata_dict=item_metadata_dict,\n",
        "    n_items=n_items,\n",
        "    K=10,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print()\n",
        "print(\"Model's performance on Train Set\\n\")\n",
        "evaluate_rank_all_batched_with_metadata(\n",
        "    model=model,\n",
        "    train_dict=train_user_items,\n",
        "    test_dict=train_user_items,\n",
        "    item_metadata_dict=item_metadata_dict,\n",
        "    n_items=n_items,\n",
        "    K=10,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tBx1182gZr8x",
      "metadata": {
        "id": "tBx1182gZr8x"
      },
      "source": [
        "## Step 7: AUC Evaluation of the Model\n",
        "In this step, we compute the Area Under the ROC Curve (AUC) to evaluate the model’s ability to distinguish between relevant (positive) and irrelevant (negative) items for each user. A higher AUC score indicates that the model is better at distinguishing between relevant and irrelevant items.\n",
        "\n",
        "For each user:\n",
        "- Positive items are those present in the user's test set (i.e., items the user has rated).\n",
        "\n",
        "- Negative items are the items not present in the test set, and a random sample of unseen items is selected as negative samples (up to a maximum of 100 items).\n",
        "\n",
        "- The model generates predicted scores for both positive and negative items, and AUC is computed based on how well the model ranks relevant items higher than irrelevant ones.\n",
        "\n",
        "AUC Calculation:\n",
        "- A user-specific AUC is computed using roc_auc_score from sklearn.metrics, which compares the predicted scores for relevant (positive) and irrelevant (negative) items.\n",
        "\n",
        "- Only users who have both positive and negative samples are included in the AUC calculation to maintain a balanced evaluation.\n",
        "\n",
        "- The final output is the mean AUC across all users, representing the model’s ability to rank relevant items higher than irrelevant ones.\n",
        "\n",
        "Key Steps:\n",
        "- Sample negative items from the pool of unseen items for each user.\n",
        "\n",
        "- Compute predicted scores for both positive (test) and negative (sampled) items.\n",
        "\n",
        "- Calculate AUC using roc_auc_score, comparing the predicted scores and true labels (1 for relevant, 0 for irrelevant).\n",
        "\n",
        "- Mean AUC is computed across all users.\n",
        "\n",
        "Output:\n",
        "- The mean AUC score represents the model's overall ability to correctly rank relevant items higher than irrelevant ones, independent of a fixed threshold.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eW5N926YJK70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW5N926YJK70",
        "outputId": "51694259-9c26-49d2-9c24-8f4f9baa6f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train AUC: 0.7582\n",
            "Test AUC: 0.8236\n"
          ]
        }
      ],
      "source": [
        "# === Compute AUC for Train Set ===\n",
        "user_aucs_train = []\n",
        "\n",
        "for user in train_user_items:\n",
        "    positives = train_user_items[user]\n",
        "    negatives = list(set(range(n_items)) - positives)\n",
        "    if len(positives) == 0 or len(negatives) == 0:\n",
        "        continue\n",
        "\n",
        "    # Sample negatives to reduce computation\n",
        "    sampled_negatives = random.sample(negatives, min(len(negatives), 100))\n",
        "    items = list(positives) + sampled_negatives\n",
        "    labels = [1] * len(positives) + [0] * len(sampled_negatives)\n",
        "\n",
        "    # Score all items for this user\n",
        "    user_tensor = torch.tensor([user] * len(items)).to(device)\n",
        "    item_tensor = torch.tensor(items).to(device)\n",
        "\n",
        "    # Metadata tensor for this batch\n",
        "    metadata_tensor = torch.stack([torch.tensor(item_metadata_dict[i]) for i in items]).float().to(device)\n",
        "\n",
        "    # Pass metadata to the model during prediction\n",
        "    scores = model(user_tensor, item_tensor, metadata_tensor).detach().cpu().numpy()\n",
        "\n",
        "    # Calculate AUC if both labels exist\n",
        "    if len(set(labels)) == 2:\n",
        "        auc = roc_auc_score(labels, scores)\n",
        "        user_aucs_train.append(auc)\n",
        "\n",
        "print(f\"Train AUC: {np.mean(user_aucs_train):.4f}\")\n",
        "\n",
        "# === Compute AUC for Test Set ===\n",
        "user_aucs = []\n",
        "\n",
        "for user in test_user_items:\n",
        "    positives = test_user_items[user]\n",
        "    negatives = list(set(range(n_items)) - train_user_items[user] - positives)\n",
        "    if len(positives) == 0 or len(negatives) == 0:\n",
        "        continue\n",
        "\n",
        "    sampled_negatives = random.sample(negatives, min(len(negatives), 100))  # or use all\n",
        "    items = list(positives) + sampled_negatives\n",
        "    labels = [1] * len(positives) + [0] * len(sampled_negatives)\n",
        "\n",
        "    user_tensor = torch.tensor([user] * len(items)).to(device)\n",
        "    item_tensor = torch.tensor(items).to(device)\n",
        "\n",
        "    # Metadata tensor for this batch\n",
        "    metadata_tensor = torch.stack([torch.tensor(item_metadata_dict[i]) for i in items]).float().to(device)\n",
        "\n",
        "    # Pass metadata to the model during prediction\n",
        "    scores = model(user_tensor, item_tensor, metadata_tensor).detach().cpu().numpy()\n",
        "\n",
        "    if len(set(labels)) == 2:\n",
        "        auc = roc_auc_score(labels, scores)\n",
        "        user_aucs.append(auc)\n",
        "\n",
        "print(f\"Test AUC: {np.mean(user_aucs):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}