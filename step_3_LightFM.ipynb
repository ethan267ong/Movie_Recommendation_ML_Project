{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BT4222 Hybrid Recommendation with LightFM\n",
        "\n",
        "## Introduction\n",
        "This notebook presents the development and evaluation of the following model:\n",
        "\n",
        "- LightFM-Based Hybrid Recommender:\n",
        "A hybrid recommendation model that combines collaborative filtering with content features using the LightFM library.\n",
        "- The model transforms user-movie ratings into interaction matrices and incorporates movie metadata (e.g., genres, director, cast) as item features.\n",
        "- It is trained using WARP (Weighted Approximate-Rank Pairwise) loss to optimize for ranking.\n",
        "- Evaluation is performed using standard and temporal split strategies, and results are reported using top-K metrics including Precision, Recall, NDCG, MAP, Hit Rate, MRR, and AUC.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "npPRw6wBWPOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "This code cell mounts your Google Drive in the Colab environment, enabling access to datasets stored in your drive for subsequent processing."
      ],
      "metadata": {
        "id": "I68IR30IQjZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJYlCv2W7k6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3730d8b-3681-42c9-ba89-04731db13e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and Load Raw Datasets\n",
        "Importing necessary libraries.\n",
        "Loading all primary datasets used in the project:\n",
        "\n",
        "- `df_tmdb_final.csv`: Final movie metadata with embeddings\n",
        "\n",
        "- `df_links_with_ratings.csv`: User ratings merged with TMDB movie IDs\n",
        "\n",
        "These datasets are merged to form `df_merged`, a comprehensive dataset that links user preferences to rich movie features."
      ],
      "metadata": {
        "id": "8vayuimNCo7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightfm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import ast\n",
        "import scipy.sparse as sp\n",
        "import re\n",
        "from scipy.sparse import csr_matrix, identity, coo_matrix\n",
        "from ast import literal_eval\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from lightfm.evaluation import precision_at_k\n",
        "from lightfm.evaluation import recall_at_k\n",
        "from lightfm.evaluation import auc_score\n",
        "from lightfm.evaluation import reciprocal_rank\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "jWGU5Tx98ki1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888aa875-c1e0-4d89-d046-a3e8b3a7ecc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.11/dist-packages (1.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lightfm) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from lightfm) (1.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from lightfm) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from lightfm) (1.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lightfm) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lightfm) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Datasets\n",
        "df_tmdb_final = pd.read_csv(\"/content/drive/MyDrive/BT4222_Project/df_tmdb_final.csv\")\n",
        "df_links_with_ratings = pd.read_csv(\"/content/drive/MyDrive/BT4222_Project/df_links_with_ratings.csv\")"
      ],
      "metadata": {
        "id": "88oYt6j8jvI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Prepare Content Feature Dataset\n",
        "We create a new dataframe df_content by selecting only the columns needed for content-based recommendation:\n",
        "\n",
        "- `movie_id`: Unique identifier for each movie  \n",
        "- `original_title`: Movie title  \n",
        "- `genre`: Genre tags (typically as lists or encoded vectors)  \n",
        "- `director_embedding`: Numeric embedding representing the movie’s director  \n",
        "- `main_cast_embeddings`: Averaged or stacked embeddings of main cast members  \n",
        "- `production_company_embedding`: Embedding of the associated production company  \n",
        "- `original_language_embedding`: Embedding for the movie's original language  \n",
        "\n",
        "\n",
        "### 1.1 String Cleaning through parse_embedding_from_array_wrapper\n",
        "When the embeddings were extracted and saved during data cleaning, they were stored as strings in the CSV. These strings sometimes include extra wrappers, such as array( ... ), and have numbers separated by spaces instead of commas—this is not a valid Python literal.\n",
        "\n",
        "- **Removing Array Wrappers**:\n",
        "It uses a regular expression to detect and remove the \"array(...)\" wrapper so that only the inner content remains.\n",
        "\n",
        "- **Ensuring Proper Bracketing**:\n",
        "It makes sure that the cleaned string starts with [ and ends with ] so that it represents a valid Python list literal.\n",
        "\n",
        "- **Inserting Commas**:\n",
        "It then uses another regular expression to insert commas between numbers that are only separated by whitespace\n",
        "\n",
        "### 1.2 Defining parse_embedding\n",
        "When the embeddings were extracted and saved during the data cleaning process, they were converted to strings in the CSV file. This makes it necessary to parse these string representations back into numerical arrays before further processing. The parse_embedding function handles this conversion by:\n",
        "\n",
        "- **Checking if the input is a string**:\n",
        "It may be a string representation of a list (e.g., \"[0.1, 0.2, 0.3]\"), so we need to convert it back to a Python list and then into a NumPy array.\n",
        "\n",
        "- **Handling errors gracefully**:\n",
        "If the string cannot be converted (due to formatting issues), the function returns a zero vector with the specified dimension as a fallback.\n",
        "\n",
        "- **Working with lists or arrays**:\n",
        "If the embedding is already in list or array format, it converts (or reaffirms) it as a NumPy array.\n"
      ],
      "metadata": {
        "id": "z0fOOdf59vkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1: Preparing the Dataset\n",
        "\n",
        "# Select relevant content-based features\n",
        "df_content = df_tmdb_final[[\n",
        "    'movie_id',\n",
        "    'original_title',\n",
        "    'genre',\n",
        "    'director_embedding',\n",
        "    'main_cast_embeddings',\n",
        "    'production_company_embedding',\n",
        "    'original_language_embedding'\n",
        "]].drop_duplicates(subset='movie_id').reset_index(drop=True)\n",
        "\n",
        "\n",
        "def parse_embedding_from_array_wrapper(s, dim=300):\n",
        "    \"\"\"\n",
        "    Convert messy string embeddings (with array(...) wrappers or irregular formatting)\n",
        "    into fixed-length NumPy vectors.\n",
        "    \"\"\"\n",
        "    if pd.isna(s):\n",
        "        return np.zeros(dim)\n",
        "\n",
        "    # Clean wrappers and whitespace\n",
        "    s = re.sub(r'array\\((.*?)\\)', r'\\1', s)      # Remove 'array(...)'\n",
        "    s = s.replace('\\n', ' ')                     # Remove newlines\n",
        "    s = s.replace('[', '').replace(']', '')      # Strip brackets\n",
        "\n",
        "    # Extract floats using regex (handles scientific notation too)\n",
        "    numbers = re.findall(r'-?\\d+\\.\\d+(?:e[+-]?\\d+)?', s)\n",
        "\n",
        "    try:\n",
        "        vec = np.array([float(n) for n in numbers], dtype=np.float32)\n",
        "\n",
        "        # Pad or truncate to target dimension\n",
        "        if len(vec) < dim:\n",
        "            vec = np.pad(vec, (0, dim - len(vec)), mode='constant')\n",
        "        elif len(vec) > dim:\n",
        "            vec = vec[:dim]\n",
        "\n",
        "        return vec\n",
        "\n",
        "    except:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "\n",
        "def parse_embedding(x, dim=300):\n",
        "    \"\"\"\n",
        "    Parses an embedding from either a valid string or an actual list/array.\n",
        "    \"\"\"\n",
        "    if isinstance(x, str):\n",
        "        x = x.strip()\n",
        "        if x.startswith('[') and x.endswith(']') and 'array' not in x:\n",
        "            try:\n",
        "                return np.array(ast.literal_eval(x))\n",
        "            except:\n",
        "                return np.zeros(dim)\n",
        "    elif isinstance(x, (list, np.ndarray)):\n",
        "        return np.array(x)\n",
        "\n",
        "    return np.zeros(dim)\n",
        "\n",
        "\n",
        "\n",
        "embedding_cols = [\n",
        "    'main_cast_embeddings',\n",
        "    'director_embedding',\n",
        "    'production_company_embedding',\n",
        "    'original_language_embedding',\n",
        "]\n",
        "\n",
        "# Apply wrapper-based parsing to messy embedding strings\n",
        "for col in embedding_cols:\n",
        "    df_content[col] = df_content[col].apply(lambda v: parse_embedding_from_array_wrapper(v, dim=300))\n",
        "\n",
        "# Apply standard parsing to clean genre embeddings\n",
        "df_content['genre'] = df_content['genre'].apply(lambda v: parse_embedding(v, dim=300))\n",
        "\n",
        "\n",
        "\n",
        "df_merged = pd.merge(df_links_with_ratings, df_content, left_on='tmdbId', right_on='movie_id', how='inner')\n",
        "del df_content"
      ],
      "metadata": {
        "id": "ta8_qDHF3not"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_cols = [\n",
        "    'main_cast_embeddings',\n",
        "    'director_embedding',\n",
        "    'production_company_embedding',\n",
        "    'original_language_embedding',\n",
        "    'genre'\n",
        "]\n",
        "\n",
        "for col in embedding_cols:\n",
        "    print(f\"{col} — valid (non-zero) vectors:\", df_content[col].apply(lambda x: np.sum(x) > 0).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMTQ-pNtkZjK",
        "outputId": "8eaab349-dbc0-4aa4-9142-7379ffcf7220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main_cast_embeddings — valid (non-zero) vectors: 268\n",
            "director_embedding — valid (non-zero) vectors: 1216\n",
            "production_company_embedding — valid (non-zero) vectors: 1341\n",
            "original_language_embedding — valid (non-zero) vectors: 1418\n",
            "genre — valid (non-zero) vectors: 1446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Building the User-Item Interaction Matrix for LightFM\n",
        "\n",
        "In this step, we convert user-movie ratings into a sparse matrix format suitable for LightFM.  \n",
        "Each user and movie is mapped to a unique matrix index. Optionally, ratings can be binarized  \n",
        "(e.g., for implicit feedback), and the final result is a CSR matrix used for model training.\n",
        "\n",
        "Outputs:\n",
        "- `interaction_matrix`: Sparse matrix of shape *(num_users × num_movies)*\n",
        "- `user_mapper`, `item_mapper`: Maps original IDs to matrix indices\n",
        "- `user_inverse_mapper`, `item_inverse_mapper`: Maps indices back to original IDs\n"
      ],
      "metadata": {
        "id": "UDp_smp8uWyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare user-item interaction matrix\n",
        "def create_interaction_matrix(df, user_col, item_col, rating_col, threshold=0):\n",
        "    \"\"\"\n",
        "    Create a user-item interaction sparse matrix\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas dataframe\n",
        "    user_col : column name for user IDs\n",
        "    item_col : column name for item IDs\n",
        "    rating_col : column name for ratings\n",
        "    threshold : minimum rating to consider as positive interaction\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    interaction_matrix : scipy sparse matrix\n",
        "    user_mapper : dict, map user ID to matrix row index\n",
        "    item_mapper : dict, map item ID to matrix column index\n",
        "    user_inverse_mapper : dict, map row index to user ID\n",
        "    item_inverse_mapper : dict, map column index to item ID\n",
        "    \"\"\"\n",
        "    # Get unique users and items\n",
        "    users = df[user_col].unique()\n",
        "    items = df[item_col].unique()\n",
        "\n",
        "    # Create mappers\n",
        "    user_mapper = {user: i for i, user in enumerate(users)}\n",
        "    item_mapper = {item: i for i, item in enumerate(items)}\n",
        "\n",
        "    user_inverse_mapper = {i: user for user, i in user_mapper.items()}\n",
        "    item_inverse_mapper = {i: item for item, i in item_mapper.items()}\n",
        "\n",
        "    # Create matrix indices\n",
        "    user_indices = [user_mapper[user] for user in df[user_col]]\n",
        "    item_indices = [item_mapper[item] for item in df[item_col]]\n",
        "\n",
        "    # Get ratings\n",
        "    ratings = [1 if r >= 4 else 0 for r in df[rating_col]]\n",
        "\n",
        "    # Create sparse matrix\n",
        "    interaction_matrix = sp.coo_matrix((ratings, (user_indices, item_indices)),\n",
        "                                       shape=(len(users), len(items)))\n",
        "\n",
        "    return interaction_matrix.tocsr(), user_mapper, item_mapper, user_inverse_mapper, item_inverse_mapper\n",
        "\n",
        "# Create the interaction matrix\n",
        "interaction_matrix, user_mapper, movie_mapper, user_inverse_mapper, movie_inverse_mapper = create_interaction_matrix(\n",
        "    df_merged, 'userId', 'movie_id', 'rating'\n",
        ")\n",
        "\n",
        "# Print out the density of the interaction matrix\n",
        "num_users, num_items = interaction_matrix.shape\n",
        "num_interactions = interaction_matrix.nnz\n",
        "sparsity = 1 - (num_interactions / (num_users * num_items))\n",
        "\n",
        "print(f\"Users: {num_users}\")\n",
        "print(f\"Items: {num_items}\")\n",
        "print(f\"Interactions: {num_interactions}\")\n",
        "print(f\"Sparsity: {sparsity:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mrQybzvwqUts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae3c8cb-2613-4e2b-eee1-14cdf1f3b43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Users: 240981\n",
            "Items: 1441\n",
            "Interactions: 6605930\n",
            "Sparsity: 0.9810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Creating the Item Features Matrix Using Embeddings and TruncatedSVD\n",
        "\n",
        "In this step, we construct a rich content-based feature matrix for movies using precomputed embedding vectors derived from metadata fields such as:\n",
        "\n",
        "- `director_embedding`\n",
        "- `main_cast_embeddings`\n",
        "- `production_company_embedding`\n",
        "- `original_language_embedding`\n",
        "- `genre`\n",
        "\n",
        "These embedding vectors are stacked horizontally into a single sparse matrix, with rows aligned to the `movie_id` ordering used in the interaction matrix.\n",
        "\n",
        "To reduce dimensionality and enhance model efficiency, we apply **TruncatedSVD** (a PCA variant for sparse matrices), projecting the combined features into a lower-dimensional latent space.\n",
        "\n",
        "**Outputs:**\n",
        "- `item_features`: A sparse matrix of shape *(num_movies × n_components)*, representing each movie's compressed content profile.\n"
      ],
      "metadata": {
        "id": "hq6iD0gku3R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_item_features(df, item_col, feature_cols, item_mapper_reference=None, n_components=None):\n",
        "    \"\"\"\n",
        "    Create item features sparse matrix for LightFM using precomputed embeddings.\n",
        "    Reduces memory usage by applying TruncatedSVD to each feature independently.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas dataframe\n",
        "    item_col : column name for item IDs\n",
        "    feature_cols : list of column names with embedding features\n",
        "    item_mapper_reference : optional dict mapping item_id -> index to align row order\n",
        "    n_components : number of dimensions to reduce each feature to (per feature)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    item_features : scipy sparse matrix (CSR)\n",
        "    item_mapper : dict mapping item_id -> row index\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Align dataframe with interaction matrix\n",
        "    if item_mapper_reference:\n",
        "        item_ids_ordered = list(item_mapper_reference.keys())\n",
        "        df = df[df[item_col].isin(item_ids_ordered)].drop_duplicates(subset=item_col).copy()\n",
        "        df['index'] = df[item_col].map(item_mapper_reference)\n",
        "        df = df.sort_values('index').drop(columns=['index'])\n",
        "        item_mapper = item_mapper_reference\n",
        "    else:\n",
        "        items = df[item_col].unique()\n",
        "        item_mapper = {item: i for i, item in enumerate(items)}\n",
        "        df = df[df[item_col].isin(item_ids_ordered)].drop_duplicates(subset=item_col).copy()\n",
        "        df['index'] = df[item_col].map(item_mapper)\n",
        "        df = df.sort_values('index').drop(columns=['index'])\n",
        "\n",
        "    # Step 2: Process and reduce each feature separately\n",
        "    reduced_matrices = []\n",
        "    for feature in feature_cols:\n",
        "        first_val = df[feature].iloc[0]\n",
        "        if isinstance(first_val, (list, np.ndarray)):\n",
        "            print(f\"Processing feature: {feature}\")\n",
        "            embeddings = np.vstack(df[feature].values).astype(np.float32)\n",
        "\n",
        "            # Apply TruncatedSVD per feature\n",
        "            if n_components and embeddings.shape[1] > n_components:\n",
        "                svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "                reduced = svd.fit_transform(embeddings)\n",
        "                reduced_sparse = sp.csr_matrix(reduced)\n",
        "            else:\n",
        "                reduced_sparse = sp.csr_matrix(embeddings)\n",
        "\n",
        "            reduced_matrices.append(reduced_sparse)\n",
        "\n",
        "    # Step 3: Combine features\n",
        "    if reduced_matrices:\n",
        "        item_features = sp.hstack(reduced_matrices).tocsr().astype(np.float32)\n",
        "    else:\n",
        "        item_features = sp.eye(len(df), format='csr', dtype=np.float32)  # fallback\n",
        "\n",
        "    return item_features, item_mapper\n",
        "\n",
        "\n",
        "# List of embedding-based feature columns\n",
        "embedding_features = [\n",
        "    'director_embedding',\n",
        "    'main_cast_embeddings',\n",
        "    'production_company_embedding',\n",
        "    'original_language_embedding',\n",
        "    'genre'\n",
        "]\n",
        "\n",
        "# Create item features matrix\n",
        "item_features, item_mapper = create_item_features(df_content, 'movie_id', embedding_features, item_mapper_reference=movie_mapper, n_components=100)\n",
        "print(f\"Item features shape: {item_features.shape}\")"
      ],
      "metadata": {
        "id": "k14yGaVnqy2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac972b5-64fa-442c-d7a2-0d23f500b0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing feature: director_embedding\n",
            "Processing feature: main_cast_embeddings\n",
            "Processing feature: production_company_embedding\n",
            "Processing feature: original_language_embedding\n",
            "Processing feature: genre\n",
            "Item features shape: (1441, 446)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation: Check for Item Mapper Alignment Between Interaction Matrix and Feature Matrix"
      ],
      "metadata": {
        "id": "MpWmm55yhzoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Same keys:\", set(movie_mapper.keys()) == set(item_mapper.keys()))\n",
        "\n",
        "# Check for mismatched indices\n",
        "for key in movie_mapper:\n",
        "    if key in item_mapper and movie_mapper[key] != item_mapper[key]:\n",
        "        print(f\"Mismatch for {key}: {movie_mapper[key]} != {item_mapper[key]}\")\n"
      ],
      "metadata": {
        "id": "hEqa6_tsg-Zo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82815ee4-7815-4395-e5d8-243a3d60f909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Same keys: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Splitting Interaction Data into Train and Test Sets\n",
        "\n",
        "To evaluate our recommendation model fairly, we split the user-item interaction matrix  \n",
        "into 80% training and 20% testing sets using `random_train_test_split`.  \n",
        "\n",
        "Ensures that All Users in the Test set exists in the Train set.\n",
        "\n",
        "Outputs:\n",
        "- `train_interactions`: Sparse matrix used for model training\n",
        "- `test_interactions`: Sparse matrix used for evaluation\n"
      ],
      "metadata": {
        "id": "hrdki6ARvGlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Split the data into train and test sets (80% train, 20% test)\n",
        "train_interactions, test_interactions = random_train_test_split(interaction_matrix, test_percentage=0.2)\n"
      ],
      "metadata": {
        "id": "gGM1WeoJrGMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Initialize and Train the LightFM Model\n",
        "\n",
        "We define a function to initialize and train a LightFM recommendation model using both collaborative signals and item-level metadata.\n",
        "\n",
        "The function supports several tunable hyperparameters:\n",
        "- `num_components`: the dimensionality of the latent space\n",
        "- `loss`: objective function (e.g., 'warp', 'bpr', etc.)\n",
        "- `epochs`: number of training iterations\n",
        "- `learning_rate`: step size for Adagrad optimization\n",
        "\n",
        "To represent users, we use an identity matrix (one-hot encoded user features). The model is trained on a sparse interaction matrix, optionally incorporating high-dimensional item features (e.g., embeddings for cast, director, production company, language).\n",
        "\n",
        "The model uses the Adagrad learning schedule and returns a trained instance of `LightFM`, suitable for evaluation and recommendation generation.\n"
      ],
      "metadata": {
        "id": "UwajgY-tvORu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Initialize and train the LightFM model\n",
        "def train_lightfm_model(train_matrix, item_features=None, num_components=20,\n",
        "                        loss='warp', epochs=15, learning_rate=0.05):\n",
        "    \"\"\"\n",
        "    Train a LightFM recommendation model\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_matrix : scipy sparse matrix of user-item interactions\n",
        "    item_features : scipy sparse matrix of item features (optional)\n",
        "    num_components : latent dimensionality of the model\n",
        "    loss : the loss function ('regression', 'bpr', 'logistic', or 'warp-kos')\n",
        "    epochs : number of training epochs\n",
        "    learning_rate : learning rate\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : trained LightFM model\n",
        "    \"\"\"\n",
        "    # Initialize the model\n",
        "    model = LightFM(no_components=num_components,\n",
        "                   loss=loss,\n",
        "                   learning_schedule='adagrad',\n",
        "                   learning_rate=learning_rate)\n",
        "\n",
        "    # Create identity user features\n",
        "    user_features = sp.identity(train_matrix.shape[0], format='csr')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_matrix,\n",
        "             user_features=user_features,\n",
        "             item_features=item_features,\n",
        "             epochs=epochs,\n",
        "             verbose=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model = train_lightfm_model(train_interactions, item_features=item_features)"
      ],
      "metadata": {
        "id": "4ngLEXiprLAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8c5ce8-7a12-4f94-f0e9-93e6d1c2ebb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 15/15 [1:48:47<00:00, 435.19s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Extended Evaluation of the LightFM Model\n",
        "\n",
        "In this step, we evaluate the trained LightFM recommendation model using a rich suite of ranking-based metrics to assess its effectiveness in top-K recommendation tasks.\n",
        "\n",
        "The evaluation supports both in-sample (training set) and out-of-sample (test set) analysis. We use both built-in LightFM metrics and custom implementations to provide a holistic performance overview.\n",
        "\n",
        "**Built-in Metrics (from LightFM):**\n",
        "- `Precision@K`: Measures the proportion of relevant items among the top-K recommendations.\n",
        "- `Recall@K`: Measures the proportion of all relevant items that were successfully recommended.\n",
        "- `AUC`: Area Under the Curve — reflects the model’s ability to rank relevant items higher than irrelevant ones.\n",
        "\n",
        "**Custom Metrics (manually computed):**\n",
        "- `Hit Rate@K`: Checks whether at least one test item is present in the top-K recommendations.\n",
        "- `MRR@K` (Mean Reciprocal Rank): Evaluates how early the first relevant item appears in the ranked list.\n",
        "- `MAP@K` (Mean Average Precision): Averages the precision scores at each position where a relevant item appears.\n",
        "- `NDCG@K` (Normalized Discounted Cumulative Gain): Considers the order of recommended items, rewarding relevant items that appear earlier.\n",
        "\n",
        "Evaluation Results:\n",
        "- Returned as a dictionary and converted to a summary DataFrame comparing performance on training and test sets.\n",
        "- Enables diagnostic insights on overfitting, cold-start issues, and real-world ranking performance.\n"
      ],
      "metadata": {
        "id": "YLSqCg2pw9wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train, test, item_features=None, k=10, allow_overlap=False):\n",
        "    \"\"\"\n",
        "    Evaluate a LightFM model using ranking metrics: Precision, Recall, AUC (using LightFM's built‑in functions)\n",
        "    and custom metrics: Hit Rate, MRR, MAP and NDCG (computed manually).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : LightFM model\n",
        "        A fitted LightFM model.\n",
        "    train : scipy sparse matrix of shape (n_users, n_items)\n",
        "        Training interactions matrix.\n",
        "    test : scipy sparse matrix of shape (n_users, n_items)\n",
        "        Test interactions matrix.\n",
        "    item_features : optional, default None\n",
        "        Item features matrix.\n",
        "    k : int, default 10\n",
        "        Number of top items to consider.\n",
        "    allow_overlap : bool, default False\n",
        "        If True, bypasses the built‑in overlap check (useful for in‑sample evaluation).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict\n",
        "        Dictionary with the computed metrics:\n",
        "          - precision: Precision@k (averaged over users)\n",
        "          - recall: Recall@k (averaged over users)\n",
        "          - auc: AUC score (averaged over users)\n",
        "          - hit_rate: Fraction of users with at least one test item in top‑k predictions\n",
        "          - mrr: Mean Reciprocal Rank over users\n",
        "          - map: Mean Average Precision over users\n",
        "          - ndcg: Normalized Discounted Cumulative Gain computed manually over users\n",
        "    \"\"\"\n",
        "    # Convert matrices to CSR format for efficient row-wise access\n",
        "    if not isinstance(train, csr_matrix):\n",
        "        train = train.tocsr()\n",
        "    if not isinstance(test, csr_matrix):\n",
        "        test = test.tocsr()\n",
        "\n",
        "    # If allowing overlap (i.e. in-sample evaluation), bypass the built-in check\n",
        "    if allow_overlap:\n",
        "        empty_train = coo_matrix(train.shape)\n",
        "        eval_train = empty_train\n",
        "    else:\n",
        "        eval_train = train\n",
        "\n",
        "    metrics = {}\n",
        "    # Compute built-in metrics from LightFM (for precision, recall, and AUC)\n",
        "    metrics[\"precision\"] = precision_at_k(\n",
        "        model, test, train_interactions=eval_train,\n",
        "        item_features=item_features, k=k\n",
        "    ).mean()\n",
        "    metrics[\"recall\"] = recall_at_k(\n",
        "        model, test, train_interactions=eval_train,\n",
        "        item_features=item_features, k=k\n",
        "    ).mean()\n",
        "    metrics[\"auc\"] = auc_score(\n",
        "        model, test, train_interactions=eval_train,\n",
        "        item_features=item_features\n",
        "    ).mean()\n",
        "\n",
        "    # Initialize lists to collect custom metric values\n",
        "    hit_rates = []\n",
        "    mrrs = []\n",
        "    average_precisions = []\n",
        "    ndcgs = []\n",
        "\n",
        "    n_users, n_items = test.shape\n",
        "\n",
        "    for user in range(n_users):\n",
        "        # Get the indices of items in the test set for this user\n",
        "        # Use proper CSR matrix row slicing\n",
        "        test_items = test[user, :].nonzero()[1]\n",
        "        if len(test_items) == 0:\n",
        "            # Skip users with no test items\n",
        "            continue\n",
        "\n",
        "        # Predict scores for all items for the user\n",
        "        scores = model.predict(user, np.arange(n_items), item_features=item_features)\n",
        "\n",
        "        # If not allowing overlap, mask (exclude) items seen in training\n",
        "        if not allow_overlap:\n",
        "            train_items = train[user, :].nonzero()[1]\n",
        "            scores[train_items] = -np.inf\n",
        "\n",
        "        # Get the top-k item indices (highest predicted scores)\n",
        "        top_k_items = np.argsort(-scores)[:k]\n",
        "\n",
        "        # --- Hit Rate ---\n",
        "        # A \"hit\" is recorded if at least one test item is in the top-k recommendations\n",
        "        hit = 1 if np.intersect1d(top_k_items, test_items).size > 0 else 0\n",
        "        hit_rates.append(hit)\n",
        "\n",
        "        # --- Mean Reciprocal Rank (MRR) ---\n",
        "        rr = 0.0\n",
        "        for rank, item in enumerate(top_k_items):\n",
        "            if item in test_items:\n",
        "                rr = 1.0 / (rank + 1)\n",
        "                break\n",
        "        mrrs.append(rr)\n",
        "\n",
        "        # --- Mean Average Precision (MAP) ---\n",
        "        num_relevant = 0\n",
        "        sum_precisions = 0.0\n",
        "        for rank, item in enumerate(top_k_items):\n",
        "            if item in test_items:\n",
        "                num_relevant += 1\n",
        "                sum_precisions += num_relevant / (rank + 1)\n",
        "        ap = sum_precisions / min(len(test_items), k)\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "        # --- Normalized Discounted Cumulative Gain (NDCG) computed manually ---\n",
        "        # Compute DCG: sum over ranked positions of (relevance / log2(rank+2))\n",
        "        dcg = 0.0\n",
        "        for rank, item in enumerate(top_k_items):\n",
        "            relevance = 1 if item in test_items else 0\n",
        "            dcg += relevance / np.log2(rank + 2)\n",
        "        # Compute the Ideal DCG (IDCG): maximum possible DCG\n",
        "        ideal_relevances = min(len(test_items), k)\n",
        "        idcg = sum([1.0 / np.log2(i + 2) for i in range(ideal_relevances)])\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "        ndcgs.append(ndcg)\n",
        "\n",
        "    # Average the custom metrics over the users that had test items\n",
        "    metrics[\"hit_rate\"] = np.mean(hit_rates) if hit_rates else 0.0\n",
        "    metrics[\"mrr\"] = np.mean(mrrs) if mrrs else 0.0\n",
        "    metrics[\"map\"] = np.mean(average_precisions) if average_precisions else 0.0\n",
        "    metrics[\"ndcg\"] = np.mean(ndcgs) if ndcgs else 0.0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# To compute in-sample (training set) metrics without causing an overlap error,\n",
        "# set allow_overlap=True so that an empty training matrix is passed to the built-in functions.\n",
        "train_metrics = evaluate_model(model, train_interactions, train_interactions,\n",
        "                               item_features=item_features, k=10, allow_overlap=True)\n",
        "\n",
        "# For out-of-sample (test set) evaluation, make sure your train/test split does not overlap:\n",
        "test_metrics = evaluate_model(model, train_interactions, test_interactions, item_features=item_features, k=10)\n",
        "\n",
        "# Combine results into a DataFrame and print them\n",
        "df_results = pd.DataFrame({\n",
        "     \"Metric\": list(train_metrics.keys()),\n",
        "     \"Train Set\": list(train_metrics.values()),\n",
        "     \"Test Set\": list(test_metrics.values())\n",
        "})\n",
        "print(df_results.to_string(index=False, float_format=\"%.4f\"))"
      ],
      "metadata": {
        "id": "ltbyD-gVmjFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8325e3f-17a8-4449-f328-f9c2646394c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Metric  Train Set  Test Set\n",
            "precision     0.3163    0.1309\n",
            "   recall     0.3598    0.2274\n",
            "      auc     0.9167    0.8781\n",
            " hit_rate     0.9481    0.5726\n",
            "      mrr     0.6423    0.2873\n",
            "      map     0.3964    0.1267\n",
            "     ndcg     0.5309    0.2064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_results.to_markdown(index=False, tablefmt=\"pipe\", floatfmt=\",.4f\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnmdlxQORcrx",
        "outputId": "fe60926e-578b-4217-816b-f8db0e112a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Metric    |   Train Set |   Test Set |\n",
            "|:----------|------------:|-----------:|\n",
            "| precision |      0.3163 |     0.1309 |\n",
            "| recall    |      0.3598 |     0.2274 |\n",
            "| auc       |      0.9167 |     0.8781 |\n",
            "| hit_rate  |      0.9481 |     0.5726 |\n",
            "| mrr       |      0.6423 |     0.2873 |\n",
            "| map       |      0.3964 |     0.1267 |\n",
            "| ndcg      |      0.5309 |     0.2064 |\n"
          ]
        }
      ]
    }
  ]
}